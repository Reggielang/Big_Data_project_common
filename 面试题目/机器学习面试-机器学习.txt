逻辑回归和线性回归首先都是广义的线性回归。
经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数。
线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。
逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，
因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。
线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。

线性回归用来预测房价，能找到一个公式来尽量拟合房价和影响房价因素之间的关系，
最后得到的公式能准确的用来预测房价。在对参数不断调优以找到一组最拟合数据的参数来构成一个最好的模型，就是线性回归。

如果我们用线性回归来预测癌症的话，对于这种分类问题，
线性回归很难找到一个完美拟合数据的模型，而且一些特别的数据，也就是噪音会对它的准确率产生很大的影响。

决策树相关问题？
常用的树搭建方法：ID3、C4.5、CART
上述几种树分别利用信息增益、信息增益率、Gini指数作为数据分割标准。

其中信息增益衡量按照某个特征分割前后熵的减少程度。

防止过拟合：剪枝
剪枝分为前剪枝和后剪枝，前剪枝本质就是早停止，后剪枝通常是通过衡量剪枝后损失函数变化来决定是否剪枝。后剪枝有：错误率降低剪枝、悲观剪枝、代价复杂度剪枝

随机森林等树算法都是非线性的，LR更侧重全局优化，而树模型主要是局部的优化。

Bagging和Boosting的区别？
1）样本选择上：
Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。
Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。
2）样例权重：
Bagging：使用均匀取样，每个样例的权重相等
Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。
3）预测函数：
Bagging：所有预测函数的权重相等。
Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。
4）并行计算：
Bagging：各个预测函数可以并行生成
Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

集成学习算法构建原理？
1.将多个分类方法聚集在一起，以提高分类的准确率（可以是相同or不同算法）
2.集成学习法由训练数据构建一组基分类器，然后通过对每个基分类器的预测进行投票来进行分类
3.严格来说，集成学习并不算是一种分类器，而是分类器结合的方法。

Boosting之AdaBoost？
Boosting的本质实际上是一个加法模型，通过改变训练样本权重学习多个分类器并进行一些线性组合。
Adaboost就是从弱分类器出发反复训练，在其中不断调整数据权重或者是概率分布，同时提高前一轮被弱分类器误分的样本的权值。最后用分类器进行投票表决（但是分类器的重要性不同）。

Boosting之GBDT？
是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终答案。
GBDT（梯度提升决策树）是为了解决一般损失函数的优化问题，
方法是用损失函数的负梯度在当前模型的值来模拟回归问题中残差（残差在数理统计中是指实际观察值与估计值（拟合值）之间的差）的近似值。
注：由于GBDT很容易出现过拟合的问题，所以推荐的GBDT深度不要超过6，而随机森林可以在15以上。

XGBoost的核心算法思想
不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数f(x)，去拟合上次预测的残差。
当我们训练完成得到k棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，
在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数
最后只需要将每棵树对应的分数加起来就是该样本的预测值。

随机森林？
基本信息：
熵越大即这个类别的不确定性更大，反之越小。
信息增益在决策树算法中是用来选择特征的指标，信息增益越大，则这个特征的选择性越好。
以决策树为基学习器构建Bagging集成，进一步在决策树的训练过程中引入随机属性选择。
对于分类问题，其输出的类别是由个别树输出的众数所决定的。在回归问题中，把每一棵决策树的输出进行平均得到最终的回归结果。
构建原理：
样本随机：从样本数据中采取有放回随机抽取N个样本
特征随机：从所有特征中随机地选取K个特征，选择最佳分割属性作为节点建立CART决策树，
重复前面的步骤，建立M棵CART树，这些树都要完全的成长且不被修剪，这些树形成了森林；
根据这些树的预测结果进行投票，决定样本的最后预测类别。
（针对回归模型，是根据这些决策树模型的平均值来获取最终的结果）
优点：
模型准确率高：随机森林既可以处理分类问题，也可以处理回归问题，即使存在部分数据缺失的情况，随机森林也能保持很高的分类精度。
能够处理数量庞大的高维度的特征，且不需要进行降维（因为特征子集是随机选择的）；
能够评估各个特征在分类问题上的重要性：可以生成树状结构，判断各个特征的重要性；
对异常值、缺失值不敏感；
随机森林有袋外数据（OOB），因此不需要单独划分交叉验证集。

常用超参数：
n_estimators：最大的弱学习器个数（建立随机森林分类器（树）的个数）。太小容易欠拟合，太大又容易过拟合，一般选择一个适中的数值。
max_features: RF划分时考虑的最大特征数。
max_depth: 决策树最大深度。
min_samples_split: 内部节点再划分所需最小样本数，默认2。

衡量分类器的好坏？
几种常用的指标：
精度precision = TP/(TP+FP) = TP/~P （~p为预测为真的数量）
召回率 recall = TP/(TP+FN) = TP/ P
F1值： 2/F1 = 1/recall + 1/precision
ROC曲线：ROC空间是一个以伪阳性率（FPR，false positive rate）为X轴，真阳性率（TPR, true positive rate）为Y轴的二维坐标系所代表的平面。其中真阳率TPR = TP / P = recall， 伪阳率FPR = FP / N
回归问题的指标：
MSE（Mean Square Error）是真实值与预测值的差值的平方然后求和平均。通过平方的形式便于求导，所以常被用作线性回归的损失函数。
RMSE（Root Mean Square Error）衡量观测值与真实值之间的偏差。 受异常点影响较大。

防止过拟合的方法？
过拟合的原因是算法的学习能力过强；训练样本过少等
处理方法：
早停止：在训练中多次迭代后发现模型性能没有显著提高就停止训练
数据集扩增：原有数据增加、原有数据加随机噪声、重采样
正则化
交叉验证
特征选择/特征降维

优化器？
梯度下降法（gradient descent）
可以分为：全量梯度下降（计算所有样本的损失），批量梯度下降（每次计算一个batch样本的损失）和随机梯度下降（每次随机选取一个样本计算损失）。
缺点：
学习率设定问题，如果学习速率过小，则会导致收敛速度很慢。如果学习速率过大，那么其会阻碍收敛，即在极值点附近会振荡。
模型所有的参数每次更新都是使用相同的学习速率。
陷入局部最小值和鞍点。
为了解决随体梯度下降上下波动，收敛速度慢的问题，提出了Momentum优化算法，这个是基于SGD的，简单理解，就是为了防止波动，取前几次波动的平均值当做这次的W。
Adam是Momentum 和 RMSprop的结合，被证明能有效适用于不同神经网络，适用于广泛的结构。是目前最常用的优化方法，优势明显。

损失函数？
1.损失函数：用来评价模型的预测值和真实值不一样的程度，损失函数越好，通常模型的性能越好。损失函数通常作为学习准则与优化问题相联系，即通过最小化损失函数求解和评估模型。
2期望风险：模型 F(x) 关于联合分布 P(X,Y) 的平均意义下的代价损失，称为风险函数（risk function）或期望损失（expected loss)
3.经验风险：模型 F(x) 关于训练数据集的平均损失，称为经验风险。当样本量趋于无穷时，经验风险趋于期望风险。
4.经验风险最小化：基于最小化平均训练误差的训练过程被称为经验风险最小化。当模型是条件概率分布，损失函数是对数损失函数的时候，经验风险最小化等价于极大似然估计。
5.结构风险最小化：指的是经验风险＋正则化项表示结构风险。是为了防止过拟合而提出来的策略。

什么是激活函数
在神经网络中，我们会对所有的输入进行加权求和，之后我们会在对结果施加一个函数，这个函数就是激活函数。
使用激活函数时为了让中间输出多样化，能够处理更复杂的问题。

如果不用激活函数的话，每一层最后输出的都是上一层输入的线性函数，不管加多少层神经网络，最后的输出也只是最开始输入数据的线性组合而已。激活函数给神经元引入了非线性因素，当加入多层神经网络时，就可以让神经网络拟合任何线性函数及非线性函数，从而使得神经网络可以适用于更多的非线性问题，而不仅仅是线性问题。

LSTM中的门控机制：
sigmoid激活函数是为了得到0/1，各门控单元{0, 1}二值输出，用于控制信息的通断。

常见激活函数？
sigmoid和softmax
sigmoid只做值非线性变化映射到(0,1)，用于二分类。
softMax变化过程计算所有结果的权重，使得多值输出的概率和为1。用于多分类。 指数运算速度慢。梯度饱和消失。
tanh函数
双曲正切函数。以0为中心，有归一化的作用。
ReLu和Leaky ReLu
大于0为1，小于0为0，计算速度快。
leaky输入为负时，梯度仍有值，避免死掉。

L1、L2正则之间有什么不同？
L2正则 对应的是2范数，使得对权重进行衰减，从而达到惩罚损失函数的目的，防止模型过拟合。保留显著减小损失函数方向上的权重，而对于那些对函数值影响不大的权重使其衰减接近于0
L1正则 对应的是1范数，同样可以防止过拟合。它会产生更稀疏的解，会使得部分权重变为0，达到特征选择的效果。

模型的精度和模型的性能哪个对你更重要？
精度只是模型性能的一部分。对于具有倾斜的数据集，比如要从大量的正常数据中识别出少量的诈骗数据，
一个精度高的模型可能会告诉你没有诈骗，然而这样的模型预测是没有意义的。

什么是F1数，怎么使用它？
F1数是衡量模型性能的一个指标。是模型精准率和召回率的加权平均，1表示最好，0表示最差。
在分类问题中有时精准率和召回率不会同时都高，那么我们可以使用F1数。

梯度消失和梯度爆炸？
当神经网络很深时，梯度呈指数级增长，最后到输入时，梯度将会非常大，我们会得到一个非常大的权重更新，这就是梯度爆炸的问题
当神经网络有很多层，每个隐藏层都使用Sigmoid函数作为激励函数时，很容易引起梯度消失的问题，Sigmoid函数有一个缺点：当x较大或较小时，导数接近0；