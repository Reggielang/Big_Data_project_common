决策树相关问题？
常用的树搭建方法：ID3、C4.5、CART
上述几种树分别利用信息增益、信息增益率、Gini指数作为数据分割标准。

其中信息增益衡量按照某个特征分割前后熵的减少程度。

防止过拟合：剪枝
剪枝分为前剪枝和后剪枝，前剪枝本质就是早停止，后剪枝通常是通过衡量剪枝后损失函数变化来决定是否剪枝。后剪枝有：错误率降低剪枝、悲观剪枝、代价复杂度剪枝

LR和随机森林区别？
随机森林等树算法都是非线性的，而LR是线性的。LR更侧重全局优化，而树模型主要是局部的优化。

Bagging和Boosting的区别？
1）样本选择上：
Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。
Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。
2）样例权重：
Bagging：使用均匀取样，每个样例的权重相等
Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。
3）预测函数：
Bagging：所有预测函数的权重相等。
Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。
4）并行计算：
Bagging：各个预测函数可以并行生成
Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

集成学习算法构建原理？
1.将多个分类方法聚集在一起，以提高分类的准确率（可以是相同or不同算法）
2.集成学习法由训练数据构建一组基分类器，然后通过对每个基分类器的预测进行投票来进行分类
3.严格来说，集成学习并不算是一种分类器，而是分类器结合的方法。

随机森林
以决策树为基学习器构建Bagging集成，进一步在决策树的训练过程中引入随机属性选择。

Boosting之AdaBoost？
Boosting的本质实际上是一个加法模型，通过改变训练样本权重学习多个分类器并进行一些线性组合。Adaboost就是从弱分类器出发反复训练，在其中不断调整数据权重或者是概率分布，同时提高前一轮被弱分类器误分的样本的权值。最后用分类器进行投票表决（但是分类器的重要性不同）。

Boosting之GBDT？
是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终答案。GBDT（梯度提升决策树）是为了解决一般损失函数的优化问题，方法是用损失函数的负梯度在当前模型的值来模拟回归问题中残差（残差在数理统计中是指实际观察值与估计值（拟合值）之间的差）的近似值。
注：由于GBDT很容易出现过拟合的问题，所以推荐的GBDT深度不要超过6，而随机森林可以在15以上。

衡量分类器的好坏？
几种常用的指标：
精度precision = TP/(TP+FP) = TP/~P （~p为预测为真的数量）
召回率 recall = TP/(TP+FN) = TP/ P
F1值： 2/F1 = 1/recall + 1/precision
ROC曲线：ROC空间是一个以伪阳性率（FPR，false positive rate）为X轴，真阳性率（TPR, true positive rate）为Y轴的二维坐标系所代表的平面。其中真阳率TPR = TP / P = recall， 伪阳率FPR = FP / N
回归问题的指标：
MSE（Mean Square Error）是真实值与预测值的差值的平方然后求和平均。通过平方的形式便于求导，所以常被用作线性回归的损失函数。
RMSE（Root Mean Square Error）衡量观测值与真实值之间的偏差。 受异常点影响较大。

防止过拟合的方法？
过拟合的原因是算法的学习能力过强；训练样本过少等
处理方法：
早停止：在训练中多次迭代后发现模型性能没有显著提高就停止训练
数据集扩增：原有数据增加、原有数据加随机噪声、重采样
正则化
交叉验证
特征选择/特征降维

优化器？
梯度下降法（gradient descent）
可以分为：全量梯度下降（计算所有样本的损失），批量梯度下降（每次计算一个batch样本的损失）和随机梯度下降（每次随机选取一个样本计算损失）。
缺点：
学习率设定问题，如果学习速率过小，则会导致收敛速度很慢。如果学习速率过大，那么其会阻碍收敛，即在极值点附近会振荡。
模型所有的参数每次更新都是使用相同的学习速率。
陷入局部最小值和鞍点。
为了解决随体梯度下降上下波动，收敛速度慢的问题，提出了Momentum优化算法，这个是基于SGD的，简单理解，就是为了防止波动，取前几次波动的平均值当做这次的W。
Adam是Momentum 和 RMSprop的结合，被证明能有效适用于不同神经网络，适用于广泛的结构。是目前最常用的优化方法，优势明显。

损失函数？
1.损失函数：用来评价模型的预测值和真实值不一样的程度，损失函数越好，通常模型的性能越好。损失函数通常作为学习准则与优化问题相联系，即通过最小化损失函数求解和评估模型。
2期望风险：模型 F(x) 关于联合分布 P(X,Y) 的平均意义下的代价损失，称为风险函数（risk function）或期望损失（expected loss)
3.经验风险：模型 F(x) 关于训练数据集的平均损失，称为经验风险。当样本量趋于无穷时，经验风险趋于期望风险。
4.经验风险最小化：基于最小化平均训练误差的训练过程被称为经验风险最小化。当模型是条件概率分布，损失函数是对数损失函数的时候，经验风险最小化等价于极大似然估计。
5.结构风险最小化：指的是经验风险＋正则化项表示结构风险。是为了防止过拟合而提出来的策略。

什么是激活函数
在神经网络中，我们会对所有的输入进行加权求和，之后我们会在对结果施加一个函数，这个函数就是激活函数。
使用激活函数时为了让中间输出多样化，能够处理更复杂的问题。

如果不用激活函数的话，每一层最后输出的都是上一层输入的线性函数，不管加多少层神经网络，最后的输出也只是最开始输入数据的线性组合而已。激活函数给神经元引入了非线性因素，当加入多层神经网络时，就可以让神经网络拟合任何线性函数及非线性函数，从而使得神经网络可以适用于更多的非线性问题，而不仅仅是线性问题。

常见激活函数？
sigmoid和softmax
sigmoid只做值非线性变化映射到(0,1)，用于二分类。
softMax变化过程计算所有结果的权重，使得多值输出的概率和为1。用于多分类。 指数运算速度慢。梯度饱和消失。
tanh函数
双曲正切函数。以0为中心，有归一化的作用。
ReLu和Leaky ReLu
大于0为1，小于0为0，计算速度快。
leaky输入为负时，梯度仍有值，避免死掉。

L1、L2正则之间有什么不同？
L2正则 对应的是2范数，使得对权重进行衰减，从而达到惩罚损失函数的目的，防止模型过拟合。保留显著减小损失函数方向上的权重，而对于那些对函数值影响不大的权重使其衰减接近于0
L1正则 对应的是1范数，同样可以防止过拟合。它会产生更稀疏的解，会使得部分权重变为0，达到特征选择的效果。

模型的精度和模型的性能哪个对你更重要？
精度只是模型性能的一部分。对于具有倾斜的数据集，比如要从大量的正常数据中识别出少量的诈骗数据，一个精度高的模型可能会告诉你没有诈骗，然而这样的模型预测是没有意义的。

什么是F1数，怎么使用它？
F1数是衡量模型性能的一个指标。是模型精准率和召回率的加权平均，1表示最好，0表示最差。在分类问题中有时精准率和召回率不会同时都高，那么我们可以使用F1数。

梯度消失和梯度爆炸？
当神经网络很深时，梯度呈指数级增长，最后到输入时，梯度将会非常大，我们会得到一个非常大的权重更新，这就是梯度爆炸的问题
当神经网络有很多层，每个隐藏层都使用Sigmoid函数作为激励函数时，很容易引起梯度消失的问题，Sigmoid函数有一个缺点：当x较大或较小时，导数接近0；